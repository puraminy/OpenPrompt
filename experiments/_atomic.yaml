dataset:
  name: atomic
  path: ../datasets/atomic # dataset in huggingface doesn't need path

plm:
  model_name: t5
  model_path: /home/ahmad/pret/t5-small
  optimize:
    freeze_para: True
    lr: 1.0e-5
    weight_decay: 0.0
    scheduler:
      type: 
      num_warmup_steps: 500

dataloader:
  max_seq_length: 256  # max_seq_length 
  decoder_max_length: 80 # the decoder max length to truncate decoder input sequence
                    # if it is an encoder-decoder architecture. Note that it's not equavalent
                    # to generation.max_length which is used merely in the generation phase.
  truncate_method: "head" # choosing from balanced, head, tail
  decode_from_pad: true

train:
  batch_size: 8
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  num_epochs:
  num_training_steps: 300


test:
  batch_size: 16

dev:
  batch_size: 16


template: soft_template
#verbalizer: manual_verbalizer



soft_template:
  choice: 0
  file_path: ../scripts/atomic/soft_template.txt
  num_tokens: 20
  initialize_from_vocab: true
  random_range: 0.5
  optimize: 
    name: AdamW
    lr: 0.3
    adam_epsilon: 1.0e-8
    scheduler:
      num_warmup_steps: 500

learning_setting: full
